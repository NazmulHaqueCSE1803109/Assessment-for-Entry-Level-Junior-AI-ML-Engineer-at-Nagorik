# -*- coding: utf-8 -*-
"""1. Build a question answering system for both bangla and english language using any open source LLM and fine tune it for your own data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mwz_pJLwChphoTfG6Rud37-rAsB1PdCt
"""

from google.colab import drive
drive.mount('/content/drive')

# First, install required packages
!pip install transformers==4.31.0 #  library developed by Hugging Face for NLP tasks. It provides pre-trained models (like GPT, BERT, T5) for tasks such as text generation, classification, translation, and summarization.
!pip install bitsandbytes==0.41.1 # A library for 8-bit model quantization, which helps reduce the memory requirements and computational load for large neural networks.
!pip install accelerate==0.21.0 #  A library by Hugging Face to make it easy to train and use models across multiple devices (e.g., CPUs, GPUs, TPUs) and in distributed setups.
!pip install peft==0.4.0 # A Hugging Face library that simplifies fine-tuning large language models efficiently without requiring extensive computational resources.
!pip install gradio # A Python library to create easy-to-use web interfaces for machine learning models or any Python functions.
!pip install torch # A deep learning framework developed by Facebook AI, widely used for building and training neural networks.
!pip install datasets
!pip install -q loralib # Implements Low-Rank Adaptation (LoRA) techniques for efficient fine-tuning of large-scale language models like GPT, BERT, etc.

# This is an ideal model and next code segment contains the tuned model.

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset, Dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import gradio as gr
import pandas as pd

class BilingualQASystem:
    def __init__(self, model_name: str = "meta-llama/Llama-2-7b-hf",
                 hf_token: str = None):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {self.device}")
        self.hf_token = hf_token

        # Configure 4-bit quantization
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )

        print("Loading model and tokenizer...")
        # Load model and tokenizer
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            trust_remote_code=True,
            token=self.hf_token,
            device_map="auto"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            token=self.hf_token,
            trust_remote_code=True
        )
        print("Model and tokenizer loaded successfully!")

        # Configure LoRA
        self.peft_config = LoraConfig(
            r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )

    def prepare_training_data(self, data_path: str):
        """
        Prepare custom dataset for fine-tuning.
        """
        print(f"Loading data from {data_path}")
        df = pd.read_csv(data_path)

        # Format data into instruction format
        def format_instruction(row):
            return f"""### Instruction: Answer the following question in {row['language']}
### Question: {row['question']}
### Answer: {row['answer']}"""

        df['text'] = df.apply(format_instruction, axis=1)

        # Convert to Hugging Face dataset
        dataset = Dataset.from_pandas(df[['text']])

        # Tokenize dataset
        def tokenize_function(examples):
            return self.tokenizer(
                examples['text'],
                truncation=True,
                max_length=512,
                padding="max_length"
            )

        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )

        print(f"Prepared {len(tokenized_dataset)} examples for training")
        return tokenized_dataset

    def fine_tune(self, train_data, output_dir: str = "/content/drive/MyDrive/qa_model_output"):
        """Fine-tune the model using LoRA"""
        print("Preparing model for training...")
        self.model = prepare_model_for_kbit_training(self.model)
        self.model = get_peft_model(self.model, self.peft_config)

        # Training arguments
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            learning_rate=2e-4,
            save_strategy="epoch",
            logging_steps=10,
            fp16=True,
        )

        # Create trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_data,
            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),
        )

        print("Starting training...")
        trainer.train()
        print(f"Training completed! Model saved to {output_dir}")

    def generate_answer(self, question: str, language: str) -> str:
        """Generate answer for a given question"""
        prompt = f"""### Instruction: Answer the following question in {language}
### Question: {question}
### Answer:"""

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id
        )

        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

# Main execution
if __name__ == "__main__":
    # Initialize system with your Hugging Face token
    qa_system = BilingualQASystem(hf_token="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")

    # Prepare and load training data from your Google Drive path
    train_data = qa_system.prepare_training_data("/content/drive/MyDrive/fine_tune_dataset/questions_and_answers.csv")

    # Fine-tune the model
    qa_system.fine_tune(train_data)

    # Create Gradio interface
    def qa_interface(question: str, language: str) -> str:
        return qa_system.generate_answer(question, language)

    interface = gr.Interface(
        fn=qa_interface,
        inputs=[
            gr.Textbox(label="Question"),
            gr.Dropdown(choices=["English", "Bengali"], label="Language")
        ],
        outputs=gr.Textbox(label="Answer"),
        title="Bilingual QA System",
        description="Ask questions in English or Bengali"
    )
    interface.launch(share=True)

# First, ensure CUDA cache is clear and set default tensor type
import torch
if torch.cuda.is_available():
    torch.cuda.empty_cache()
torch.set_default_tensor_type(torch.FloatTensor)

# Disable wandb
import os
os.environ["WANDB_DISABLED"] = "true"

# # Install required packages
# !pip install transformers==4.31.0
# !pip install peft==0.4.0
# !pip install gradio
# !pip install torch
# !pip install datasets
# !pip install -q loralib

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from transformers import (
    T5ForConditionalGeneration,
    T5TokenizerFast,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq
)
from datasets import Dataset
from peft import LoraConfig, get_peft_model
import gradio as gr

class BilingualQASystem:
    def __init__(self, model_name: str = "t5-small", hf_token: str = None): # Initializes the model, tokenizer, and LoRA (Low-Rank Adaptation) configuration.
        """Initialize with smallest T5 model"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu" # Checks if a GPU is available (torch.cuda.is_available()); if so, sets the device to "cuda", otherwise "cpu".
        print(f"Using device: {self.device}")
        self.hf_token = hf_token # Stores the Hugging Face token for model downloads.

        # Load model with explicit precision control
        print(f"Loading model: {model_name}")
        self.model = T5ForConditionalGeneration.from_pretrained(
            model_name,                    # Downloads and loads the specified model ("t5-small" by default).
            torch_dtype=torch.float32,     # Ensures the model uses 32-bit floating point precision.
            low_cpu_mem_usage=True,        # Optimizes memory usage for CPU during model loading.
            token=self.hf_token,           # Allows loading models with custom code.
            trust_remote_code=True
        )
        self.model = self.model.to(self.device) #  Moves the model to the specified device (GPU or CPU).

        #  Loads a tokenizer corresponding to the model (T5TokenizerFast), which handles text tokenization.
        print("Loading tokenizer...")
        self.tokenizer = T5TokenizerFast.from_pretrained(
            model_name,
            token=self.hf_token
        )

        print("Model and tokenizer loaded successfully!")

        # Configures LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning.
        self.peft_config = LoraConfig(
            r=4,  # Low-rank approximation dimension.
            lora_alpha=16,   # Scaling factor for LoRA updates.
            target_modules=["q", "v"], # Specifies which modules in the model to fine-tune (query and value layers in attention).
            lora_dropout=0.05, # Adds a small dropout for robustness.
            bias="none", # Specifies that no additional bias parameters will be added.
            task_type="SEQ_2_SEQ_LM"
        )

    def prepare_training_data(self, data_path: str): # Loads training data from a CSV file and prepares it for model training.
        print(f"Loading data from: {data_path}")
        df = pd.read_csv(data_path)

        # Prepare data dictionary
        dataset_dict = {
            "input_text": [
                f"translate to {row['language']}: {row['question']}"
                for _, row in df.iterrows()
            ],
            "labels": df['answer'].tolist()
        }

        # Converts the dictionary to a Hugging Face Dataset object for easier processing.
        dataset = Dataset.from_dict(dataset_dict)

        # Encodes input_text into token IDs with padding, truncation, and a maximum length of 64.
        def tokenize_data(examples):
            inputs = self.tokenizer(
                examples["input_text"],
                padding=True,
                truncation=True,
                max_length=64,
                return_tensors=None
            )

            targets = self.tokenizer(
                examples["labels"],
                padding=True,
                truncation=True,
                max_length=64,
                return_tensors=None
            )

            inputs["labels"] = targets["input_ids"]
            return inputs

        tokenized_dataset = dataset.map(tokenize_data, remove_columns=dataset.column_names)
        # Applies the tokenize_data function to the dataset using the map method.
        # remove_columns=dataset.column_names: Drops original columns after tokenization.

        print(f"Prepared {len(tokenized_dataset)} examples for training")
        return tokenized_dataset

    def fine_tune(self, train_data, output_dir: str = "/content/drive/MyDrive/qa_model_output"): # A method to fine-tune the model on a given dataset (train_data) and save the fine-tuned model to the specified output_dir.
        print("Preparing model for fine-tuning...")

        # A function (likely from the PEFT library) to apply parameter-efficient fine-tuning (PEFT) to the model.
        self.model = get_peft_model(self.model, self.peft_config) # Prepares the model to use techniques like LoRA, which reduces the number of trainable parameters and memory usage.

        # Training arguments with explicit precision control
        training_args = TrainingArguments(  # TrainingArguments: A Hugging Face utility that specifies hyperparameters and settings for training.
            output_dir=output_dir,  #  Directory to save the fine-tuned model.
            num_train_epochs=3, #  Number of training epochs (3 in this case).
            per_device_train_batch_size=2, #  Batch size per device (2 examples per GPU/CPU).
            gradient_accumulation_steps=2, # Number of steps to accumulate gradients before backpropagation, effectively increasing the batch size.
            learning_rate=5e-5, # Learning rate for optimization (5e-5 is a typical starting point for fine-tuning).
            save_strategy="no", # Indicates saving strategy (set to "no", meaning no intermediate checkpoints).
            logging_steps=1, # Frequency of logging training metrics (every 1 step here).
            fp16=False, # Flags to enable half-precision (float16) or bfloat16 training for performance optimization. Both are disabled (False).
            bf16=False,
            optim="adamw_torch", # Optimization algorithm used ("adamw_torch"), which is Adam with weight decay.
            report_to="none",
            max_grad_norm=0.3,
            warmup_ratio=0.1,
            remove_unused_columns=False
        )

        # Create trainer
        trainer = Trainer( #  Initializes a Hugging Face Trainer for managing the training process.
            model=self.model, # The prepared model (self.model) for fine-tuning.
            args=training_args, # The training arguments (training_args) defined above.
            train_dataset=train_data, # The training dataset (train_data) used to fine-tune the model.
            data_collator=DataCollatorForSeq2Seq(
                self.tokenizer,
                pad_to_multiple_of=None,
                return_tensors="pt"
            )
        )

        print("Starting training...")
        trainer.train()

        print(f"Saving model to {output_dir}")
        self.model.save_pretrained(output_dir)

    def generate_answer(self, question: str, language: str) -> str:  #  Generates an answer for a given question in the specified language using the fine-tuned model.
        input_text = f"translate to {language}: {question}"
        # The input question to answer.
        # The target language for the response (e.g., "English" or "Bengali").

        # Tokenizes the input text using the preloaded tokenizer.
        inputs = self.tokenizer(
            input_text,
            padding=True,
            truncation=True,
            max_length=64,
            return_tensors="pt"
        ).to(self.device)

        try:
            # Generate with conservative settings
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=64, # Limits the length of the generated text.
                    num_beams=2,   # Enables beam search with two beams for better answers.
                    temperature=0.7,  # Controls randomness (lower values produce less random responses).
                    no_repeat_ngram_size=2  # Avoids repeating sequences of 2 or more words.
                )

            return self.tokenizer.decode(outputs[0], skip_special_tokens=True) # Converts the generated token IDs back into a human-readable string.

        except Exception as e:
            return f"Error generating answer: {str(e)}"

def main():
    try:
        # Initialize system with your Hugging Face token
        qa_system = BilingualQASystem(hf_token="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")

        # Load and prepare data
        data_path = "/content/drive/MyDrive/Fine Tune DataSet/questions_and_answers.csv"
        train_data = qa_system.prepare_training_data(data_path)
        # Loads a CSV file containing questions, answers, and target languages.
        # Converts the data into a tokenized format suitable for training.

        # Fine-tunes the base model using the prepared training data.
        qa_system.fine_tune(train_data)

        # Create interface Bilingual QA System
        interface = gr.Interface(
            fn=lambda q, l: qa_system.generate_answer(q, l),
            inputs=[
                gr.Textbox(label="Question"),
                gr.Dropdown(choices=["English", "Bengali"], label="Language")
            ],
            outputs=gr.Textbox(label="Answer"),
            title="Bilingual QA System",
            description="Ask questions in English or Bengali"
        )

        interface.launch(share=True)

    except Exception as e:
        print(f"Error in main execution: {str(e)}")

if __name__ == "__main__":
    main()